{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93e69eaa",
   "metadata": {},
   "source": [
    "# Figures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c343da8",
   "metadata": {},
   "source": [
    "----------------\n",
    "\n",
    "We generate all the figures and statistics that appear in the paper here.\n",
    "\n",
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488a67a0",
   "metadata": {},
   "source": [
    "## 1) Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59bfc376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../../model\")\n",
    "from model_params import *\n",
    "\n",
    "sys.path.append(\"../../data\")\n",
    "from dataset_params import *\n",
    "from prompt_params import *\n",
    "from demo_params import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f07d7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    \"gpt2_xl\", \n",
    "    \"gpt_j\", \n",
    "    \"gpt_neox_20B\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d6b36f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    \"sst2\",\n",
    "    \"agnews\",\n",
    "    \"trec\",\n",
    "    \"dbpedia\",\n",
    "    \"rte\",\n",
    "    \"mrpc\",\n",
    "    \"tweet_eval_hate\",\n",
    "    \"sick\",\n",
    "    \"poem_sentiment\",\n",
    "    \"ethos\",\n",
    "    \"financial_phrasebank\",\n",
    "    \"medical_questions_pairs\",\n",
    "    \"tweet_eval_stance_feminist\",\n",
    "    \"tweet_eval_stance_atheism\",\n",
    "    \"unnatural\",\n",
    "    \"sst2_ab\",\n",
    "]\n",
    "\n",
    "# non-synthetic tasks\n",
    "datasets_main = [\n",
    "    \"sst2\",\n",
    "    \"agnews\",\n",
    "    \"trec\",\n",
    "    \"dbpedia\",\n",
    "    \"rte\",\n",
    "    \"mrpc\",\n",
    "    \"tweet_eval_hate\",\n",
    "    \"sick\",\n",
    "    \"poem_sentiment\",\n",
    "    \"ethos\",\n",
    "    \"financial_phrasebank\",\n",
    "    \"medical_questions_pairs\",\n",
    "    \"tweet_eval_stance_feminist\",\n",
    "    \"tweet_eval_stance_atheism\",\n",
    "]\n",
    "\n",
    "# datasets with 3 or more labels\n",
    "datasets_3_plus = [\n",
    "    \"agnews\",\n",
    "    \"trec\",\n",
    "    \"dbpedia\",\n",
    "    \"sick\",\n",
    "    \"poem_sentiment\",\n",
    "    \"financial_phrasebank\",\n",
    "    \"tweet_eval_stance_feminist\",\n",
    "    \"tweet_eval_stance_atheism\",\n",
    "    \"unnatural\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d960e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = [\n",
    "    \"permuted_incorrect_labels\",\n",
    "    \"half_permuted_incorrect_labels\",\n",
    "    \"random_labels\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc08cb96",
   "metadata": {},
   "source": [
    "## 2) Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f14d0331",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_lens = {}\n",
    "for model in models:\n",
    "    logit_lens[model] = {}\n",
    "    for setting in settings:\n",
    "        logit_lens[model][setting] = {}\n",
    "        for dataset in datasets:\n",
    "            logit_lens[model][setting][dataset] = np.load(\n",
    "                f\"../../results/logit_lens/{model}/{setting}/{dataset}.npy\",\n",
    "                allow_pickle=True,\n",
    "            ).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ae7da7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = {}\n",
    "dataset = \"unnatural\"\n",
    "for model in models:\n",
    "    attention[model] = {}\n",
    "    for setting in settings:\n",
    "        attention[model][setting] = {}\n",
    "        attention[model][setting][dataset] = data_frame = pd.read_csv(f\"../../results/attention/{model}/{dataset}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f67523",
   "metadata": {},
   "source": [
    "## 3) Layerwise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec47a310",
   "metadata": {},
   "source": [
    "### 3.1) SST-2 Prompt Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "166f485f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_prompt_formats(models, settings, metrics, datasets):\n",
    "    for model in models:\n",
    "        n_layers = MODEL_PARAMS[model][\"num_layers\"] + 1\n",
    "        for setting in settings:\n",
    "            for metric in metrics:\n",
    "                for dataset in datasets:\n",
    "                    prompts = logit_lens[model][setting][dataset][metric]\n",
    "                    for j, prompt in enumerate(prompts):\n",
    "                        tp_df = pd.DataFrame({\"layer\": list(range(n_layers)), \"p\": prompt[0][0, :, -1]})\n",
    "                        fp_df = pd.DataFrame({\"layer\": list(range(n_layers)), \"p\": prompt[0][1, :, -1]})\n",
    "                        zero_shot = pd.DataFrame({\"layer\": list(range(n_layers)), \"p\": prompt[0][0, :, 0]})\n",
    "                        tp_df.to_csv(\n",
    "                            f\"../../results/figures/layerwise/{model}/{setting}/{metric}/{dataset}_prompt_format_id_{j}_true_prefix.csv\",\n",
    "                            index=False,\n",
    "                        )\n",
    "                        fp_df.to_csv(\n",
    "                            f\"../../results/figures/layerwise/{model}/{setting}/{metric}/{dataset}_prompt_format_id_{j}_false_prefix.csv\",\n",
    "                            index=False,\n",
    "                        )\n",
    "                        zero_shot.to_csv(\n",
    "                            f\"../../results/figures/layerwise/{model}/{setting}/{metric}/{dataset}_prompt_format_id_{j}_zero_shot.csv\",\n",
    "                            index=False,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87e2b95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_prompt_formats([\"gpt_j\"], [\"permuted_incorrect_labels\"], [\"cal_correct_over_incorrect\"], [\"sst2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28474e4c",
   "metadata": {},
   "source": [
    "### 3.2) Average over Prompt Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8bc3971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_avg_over_pf_baseline(models, settings, metrics, datasets):\n",
    "    for model in models:\n",
    "        n_layers = MODEL_PARAMS[model][\"num_layers\"] + 1\n",
    "        for setting in settings:\n",
    "            for metric in metrics:\n",
    "                for dataset in datasets:\n",
    "                    n_labels = len(PROMPT_PARAMS[dataset][0][\"labels\"])\n",
    "                    baseline_data = {\"layer\": list(range(n_layers)), \"p\": [1 / n_labels] * n_layers}\n",
    "                    baseline_df = pd.DataFrame(baseline_data)\n",
    "                    baseline_df.to_csv(\n",
    "                        f\"../../results/figures/layerwise/{model}/{setting}/{metric}/{dataset}_baseline.csv\",\n",
    "                        index=False,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10732dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_avg_over_pf_baseline(models, [\"permuted_incorrect_labels\"], [\"cal_correct_over_incorrect\"], datasets)\n",
    "save_avg_over_pf_baseline([\"gpt_j\"], [\"permuted_incorrect_labels\"], [\"correct_over_incorrect\"], datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0baeb372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_avg_over_pf(models, settings, metrics, datasets, save_tp, save_fp, save_zero_shot):\n",
    "    for model in models:\n",
    "        n_layers = MODEL_PARAMS[model][\"num_layers\"] + 1\n",
    "        for setting in settings:\n",
    "            for metric in metrics:\n",
    "                for dataset in datasets:\n",
    "                    avg = np.mean(logit_lens[model][setting][dataset][metric], axis=0)[0]\n",
    "                    tp_df = pd.DataFrame({\"layer\": list(range(n_layers)), \"p\": avg[0, :, -1]})\n",
    "                    fp_df = pd.DataFrame({\"layer\": list(range(n_layers)), \"p\": avg[1, :, -1]})\n",
    "                    zero_shot = pd.DataFrame({\"layer\": list(range(n_layers)), \"p\": avg[0, :, 0]})\n",
    "                    if save_tp:\n",
    "                        tp_df.to_csv(\n",
    "                            f\"../../results/figures/layerwise/{model}/{setting}/{metric}/{dataset}_true_prefix.csv\",\n",
    "                            index=False,\n",
    "                        )\n",
    "                    if save_fp:\n",
    "                        fp_df.to_csv(\n",
    "                            f\"../../results/figures/layerwise/{model}/{setting}/{metric}/{dataset}_false_prefix.csv\",\n",
    "                            index=False,\n",
    "                        )\n",
    "                    if save_zero_shot:\n",
    "                        zero_shot.to_csv(\n",
    "                            f\"../../results/figures/layerwise/{model}/{setting}/{metric}/{dataset}_zero_shot.csv\",\n",
    "                            index=False,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97759b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_avg_over_pf(models, [\"permuted_incorrect_labels\"], [\"cal_correct_over_incorrect\"], datasets, True, True, True)\n",
    "save_avg_over_pf(models, [\"half_permuted_incorrect_labels\", \"random_labels\"], [\"cal_correct_over_incorrect\"], datasets, False, True, False)\n",
    "save_avg_over_pf([\"gpt_j\"], [\"permuted_incorrect_labels\"], [\"correct_over_incorrect\"], datasets, True, True, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0714a41d",
   "metadata": {},
   "source": [
    "### 3.3) Average over Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07dba553",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_avg_over_datasets_baseline(models, settings, metrics, datasets):\n",
    "    for model in models:\n",
    "        n_layers = MODEL_PARAMS[model][\"num_layers\"] + 1\n",
    "        for setting in settings:\n",
    "            for metric in metrics:\n",
    "                baseline = 0\n",
    "                for dataset in datasets:\n",
    "                    baseline += 1 / len(PROMPT_PARAMS[dataset][0][\"labels\"])\n",
    "                baseline /= len(datasets)\n",
    "\n",
    "                if metric == \"top_1_acc\" or metric == \"label_space_probs\":\n",
    "                    baseline = 1 / 50400\n",
    "                elif metric == \"cal_permute\":\n",
    "                    baseline = 1\n",
    "                baseline_data = {\"layer\": list(range(n_layers)), \"p\": [baseline] * n_layers}\n",
    "                baseline_df = pd.DataFrame(baseline_data)\n",
    "                baseline_df.to_csv(\n",
    "                    f\"../../results/figures/layerwise/{model}/{setting}/{metric}/average_baseline.csv\",\n",
    "                    index=False,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "778cf93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_avg_over_datasets_baseline(models, [\"permuted_incorrect_labels\"], [\"cal_correct_over_incorrect\"], datasets_main)\n",
    "save_avg_over_datasets_baseline(models, [\"permuted_incorrect_labels\"], [\"correct_over_incorrect\", \"top_1_acc\", \"cal_permute\", \"label_space_probs\"], datasets_main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68ddd38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_avg_over_datasets(models, settings, metrics, datasets, save_tp, save_fp, save_zero_shot):\n",
    "    for model in models:\n",
    "        n_layers = MODEL_PARAMS[model][\"num_layers\"] + 1\n",
    "        for setting in settings:\n",
    "            for metric in metrics:\n",
    "                tp_avg = np.zeros(n_layers)\n",
    "                fp_avg = np.zeros(n_layers)\n",
    "                zero_shot_avg = np.zeros(n_layers)\n",
    "                for dataset in datasets:\n",
    "                    avg = np.mean(logit_lens[model][setting][dataset][metric], axis=0)[0]\n",
    "                    tp = avg[0, :, -1]\n",
    "                    fp = avg[1, :, -1]\n",
    "                    zs = avg[0, :, 0]\n",
    "                    if metric == \"cal_permute\":\n",
    "                        tp *= len(PROMPT_PARAMS[dataset][0][\"labels\"])\n",
    "                        fp *= len(PROMPT_PARAMS[dataset][0][\"labels\"])\n",
    "                        zs *= len(PROMPT_PARAMS[dataset][0][\"labels\"])\n",
    "                    tp_avg += tp\n",
    "                    fp_avg += fp\n",
    "                    zero_shot_avg += zs\n",
    "                tp_avg /= len(datasets)\n",
    "                fp_avg /= len(datasets)\n",
    "                zero_shot_avg /= len(datasets)\n",
    "                tp_df = pd.DataFrame({\"layer\": list(range(n_layers)), \"p\": tp_avg})\n",
    "                fp_df = pd.DataFrame({\"layer\": list(range(n_layers)), \"p\": fp_avg})\n",
    "                zero_shot = pd.DataFrame({\"layer\": list(range(n_layers)), \"p\": zero_shot_avg})\n",
    "                if save_tp:\n",
    "                    tp_df.to_csv(\n",
    "                        f\"../../results/figures/layerwise/{model}/{setting}/{metric}/average_true_prefix.csv\",\n",
    "                        index=False,\n",
    "                    )\n",
    "                if save_fp:\n",
    "                    fp_df.to_csv(\n",
    "                        f\"../../results/figures/layerwise/{model}/{setting}/{metric}/average_false_prefix.csv\",\n",
    "                        index=False,\n",
    "                    )\n",
    "                if save_zero_shot:\n",
    "                    zero_shot.to_csv(\n",
    "                        f\"../../results/figures/layerwise/{model}/{setting}/{metric}/average_zero_shot.csv\",\n",
    "                        index=False,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3f625cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_avg_over_datasets(models, [\"permuted_incorrect_labels\"], [\"cal_correct_over_incorrect\"], datasets_main, True, True, True)\n",
    "save_avg_over_datasets(models, [\"half_permuted_incorrect_labels\", \"random_labels\"], [\"cal_correct_over_incorrect\"], datasets_main, False, True, False)\n",
    "save_avg_over_datasets(models, [\"permuted_incorrect_labels\"], [\"correct_over_incorrect\", \"top_1_acc\", \"cal_permute\", \"label_space_probs\"], datasets_main, True, True, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7a6f2a",
   "metadata": {},
   "source": [
    "## 4) Contextwise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7f5d37",
   "metadata": {},
   "source": [
    "### 4.1) Accuracy Gap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c135008",
   "metadata": {},
   "source": [
    "#### 4.1.1) Average over Prompt Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a150653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_acc_gap_avg_over_pf(models, settings, metrics, datasets):\n",
    "    for model in models:\n",
    "        n_layers = MODEL_PARAMS[model][\"num_layers\"] + 1\n",
    "        n_demos = MODEL_PARAMS[model][\"max_demos\"]\n",
    "        for setting in settings:\n",
    "            for metric in metrics:\n",
    "                for dataset in datasets:\n",
    "                    avg = np.mean(logit_lens[model][setting][dataset][metric], axis=0)[0]\n",
    "                    acc_gap = avg[0, -1, :] - avg[1, -1, :]\n",
    "                    acc_gap_df = pd.DataFrame({\"pic\": list(range(n_demos)), \"p\": acc_gap})\n",
    "                    acc_gap_df.to_csv(\n",
    "                        f\"../../results/figures/contextwise/{model}/{setting}/acc_gap/{dataset}.csv\",\n",
    "                        index=False,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4cfa6152",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_acc_gap_avg_over_pf(models, [\"permuted_incorrect_labels\"], [\"cal_correct_over_incorrect\"], datasets_main)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40216a44",
   "metadata": {},
   "source": [
    "#### 4.1.2) Average over Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea96d955",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_acc_gap_avg_over_datasets(models, settings, metrics, datasets):\n",
    "    for model in models:\n",
    "        n_layers = MODEL_PARAMS[model][\"num_layers\"] + 1\n",
    "        n_demos = MODEL_PARAMS[model][\"max_demos\"]\n",
    "        for setting in settings:\n",
    "            for metric in metrics:\n",
    "                acc_gap = np.zeros(n_demos)\n",
    "                for dataset in datasets:\n",
    "                    avg = np.mean(logit_lens[model][setting][dataset][metric], axis=0)[0]\n",
    "                    acc_gap += avg[0, -1, :] - avg[1, -1, :]\n",
    "                acc_gap /= len(datasets)\n",
    "                acc_gap_df = pd.DataFrame({\"pic\": list(range(n_demos)), \"p\": acc_gap})\n",
    "                acc_gap_df.to_csv(\n",
    "                    f\"../../results/figures/contextwise/{model}/{setting}/acc_gap/average.csv\",\n",
    "                    index=False,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02b362d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_acc_gap_avg_over_datasets(models, [\"permuted_incorrect_labels\"], [\"cal_correct_over_incorrect\"], datasets_main)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbac8551",
   "metadata": {},
   "source": [
    "### 4.2) Permute Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2feaded0",
   "metadata": {},
   "source": [
    "#### 4.2.1) Average over Prompt Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d3e383db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_permute_score_avg_over_pf(models, settings, datasets):\n",
    "    metric = \"cal_permute\"\n",
    "    for model in models:\n",
    "        n_layers = MODEL_PARAMS[model][\"num_layers\"] + 1\n",
    "        n_demos = MODEL_PARAMS[model][\"max_demos\"]\n",
    "        for setting in settings:\n",
    "            for dataset in datasets:\n",
    "                avg = np.mean(logit_lens[model][setting][dataset][metric], axis=0)[0]\n",
    "                permute_score = avg[1, -1, :] * len(PROMPT_PARAMS[dataset][0][\"labels\"])\n",
    "                permute_score_df = pd.DataFrame({\"pic\": list(range(n_demos)), \"p\": permute_score})\n",
    "                permute_score_df.to_csv(\n",
    "                    f\"../../results/figures/contextwise/{model}/{setting}/permute_score/{dataset}.csv\",\n",
    "                    index=False,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "34482141",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_permute_score_avg_over_pf([\"gpt_j\"], [\"permuted_incorrect_labels\"], datasets_3_plus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe68c80e",
   "metadata": {},
   "source": [
    "#### 4.2.2) Average over Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3fe5a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_permute_score_avg_over_datasets(models, settings, datasets):\n",
    "    metric = \"cal_permute\"\n",
    "    for model in models:\n",
    "        n_layers = MODEL_PARAMS[model][\"num_layers\"] + 1\n",
    "        n_demos = MODEL_PARAMS[model][\"max_demos\"]\n",
    "        for setting in settings:\n",
    "            permute_score = np.zeros(n_demos)\n",
    "            for dataset in datasets:\n",
    "                avg = np.mean(logit_lens[model][setting][dataset][metric], axis=0)[0]\n",
    "                permute_score += avg[1, -1, :] * len(PROMPT_PARAMS[dataset][0][\"labels\"])\n",
    "            permute_score /= len(datasets)\n",
    "            permute_score_df = pd.DataFrame({\"pic\": list(range(n_demos)), \"p\": permute_score})\n",
    "            permute_score_df.to_csv(\n",
    "                f\"../../results/figures/contextwise/{model}/{setting}/permute_score/average.csv\",\n",
    "                index=False,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a8d47f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_permute_score_avg_over_datasets([\"gpt_j\"], [\"permuted_incorrect_labels\"], datasets_3_plus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceef4038",
   "metadata": {},
   "source": [
    "## 5) Gap Appearance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd01d95",
   "metadata": {},
   "source": [
    "see notebooks/analysis/logit_lens\n",
    "\n",
    "**gpt2_xl** \\\n",
    "Layer: 22. Dataset: sst2. \\\n",
    "Layer: 22. Dataset: poem_sentiment. \\\n",
    "Layer: 23. Dataset: financial_phrasebank. \\\n",
    "Layer: 23. Dataset: ethos. \\\n",
    "Layer: 21. Dataset: tweet_eval_hate. \\\n",
    "Layer: 42. Dataset: tweet_eval_stance_atheism. \\\n",
    "Layer: 24. Dataset: tweet_eval_stance_feminist. \\\n",
    "Layer: 03. Dataset: medical_questions_pairs. \\\n",
    "Layer: 03. Dataset: mrpc. \\\n",
    "Layer: 21. Dataset: sick. \\\n",
    "Layer: 20. Dataset: rte. \\\n",
    "Layer: 23. Dataset: agnews. \\\n",
    "Layer: 22. Dataset: trec. \\\n",
    "Layer: 24. Dataset: dbpedia.\n",
    "\n",
    "**gpt_j** \\\n",
    "Layer: 14. Dataset: sst2. \\\n",
    "Layer: 14. Dataset: poem_sentiment. \\\n",
    "Layer: 14. Dataset: financial_phrasebank. \\\n",
    "Layer: 14. Dataset: ethos. \\\n",
    "Layer: 14. Dataset: tweet_eval_hate. \\\n",
    "Layer: 14. Dataset: tweet_eval_stance_atheism. \\\n",
    "Layer: 14. Dataset: tweet_eval_stance_feminist. \\\n",
    "Layer: 13. Dataset: medical_questions_pairs. \\\n",
    "Layer: 13. Dataset: mrpc. \\\n",
    "Layer: 13. Dataset: sick. \\\n",
    "Layer: 14. Dataset: rte. \\\n",
    "Layer: 17. Dataset: agnews. \\\n",
    "Layer: 14. Dataset: trec. \\\n",
    "Layer: 11. Dataset: dbpedia.\n",
    "\n",
    "**gpt_neox** \\\n",
    "Layer: 10. Dataset: sst2. \\\n",
    "Layer: 12. Dataset: poem_sentiment. \\\n",
    "Layer: 10. Dataset: financial_phrasebank. \\\n",
    "Layer: 10. Dataset: ethos. \\\n",
    "Layer: 10. Dataset: tweet_eval_hate. \\\n",
    "Layer: 10. Dataset: tweet_eval_stance_atheism. \\\n",
    "Layer: 10. Dataset: tweet_eval_stance_feminist. \\\n",
    "Layer: 13. Dataset: medical_questions_pairs. \\\n",
    "Layer: 04. Dataset: mrpc. \\\n",
    "Layer: 10. Dataset: sick. \\\n",
    "Layer: 02. Dataset: rte. \\\n",
    "Layer: 10. Dataset: agnews. \\\n",
    "Layer: 10. Dataset: trec. \\\n",
    "Layer: 12. Dataset: dbpedia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eab835b",
   "metadata": {},
   "source": [
    "## 6) Early Exiting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77427199",
   "metadata": {},
   "source": [
    "see notebooks/analysis/logit_lens\n",
    "\n",
    "We identified the following critical layers:\n",
    "\n",
    "**gpt2_xl** \\\n",
    "Layer 30\n",
    "\n",
    "**gpt_j** \\\n",
    "Layer 16\n",
    "\n",
    "**gpt_neox** \\\n",
    "Layer 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e0fe26",
   "metadata": {},
   "source": [
    "### 6.1) Critical Layer Early Exiting Success"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70615a3",
   "metadata": {},
   "source": [
    "see notebooks/analysis/logit_lens\n",
    "\n",
    "The number of datasets where early exiting is better than the final layer.\n",
    "\n",
    "**gpt2_xl** \\\n",
    "\\# of Datasets: 10\n",
    "\n",
    "**gpt_j** \\\n",
    "\\# of Datasets: 14\n",
    "\n",
    "**gpt_neox** \\\n",
    "\\# of Datasets: 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6a3a64",
   "metadata": {},
   "source": [
    "### 6.2) Critical Layer Early Exiting Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4586d46",
   "metadata": {},
   "source": [
    "see notebooks/analysis/logit_lens\n",
    "\n",
    "The performance differrence between early exiting and full evaluation.\n",
    "\n",
    "Setting: **permuted_incorrect_labels** \\\n",
    "Model: **gpt2_xl** \\\n",
    "True Prefix Delta: -0.517% \\\n",
    "False Prefix Delta: 2.658% \\\n",
    "Model: **gpt_j** \\\n",
    "True Prefix Delta: -1.538% \\\n",
    "False Prefix Delta: 10.299% \\\n",
    "Model: **gpt_neox** \\\n",
    "True Prefix Delta: 0.477% \\\n",
    "False Prefix Delta: 0.869%\n",
    "\n",
    "Setting: **half_permuted_incorrect_labels** \\\n",
    "Model: **gpt2_xl** \\\n",
    "True Prefix Delta: 0.070% \\\n",
    "False Prefix Delta: 1.573% \\\n",
    "Model: **gpt_j** \\\n",
    "True Prefix Delta: -2.544% \\\n",
    "False Prefix Delta: 2.607% \\\n",
    "Model: **gpt_neox** \\\n",
    "True Prefix Delta: 0.033% \\\n",
    "False Prefix Delta: 0.150%\n",
    "\n",
    "Setting: **random_labels** \\\n",
    "Model: **gpt2_xl** \\\n",
    "True Prefix Delta: 0.209% \\\n",
    "False Prefix Delta: 2.329% \\\n",
    "Model: **gpt_j** \\\n",
    "True Prefix Delta: -1.931% \\\n",
    "False Prefix Delta: 4.604% \\\n",
    "Model: **gpt_neox** \\\n",
    "True Prefix Delta: 0.616% \\\n",
    "False Prefix Delta: 1.320%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169fa743",
   "metadata": {},
   "source": [
    "## 7) Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53870a53",
   "metadata": {},
   "source": [
    "#### 7.1) Layerwise Percent of Final Gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "23ae673b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_perc_of_gap_avg_over_datasets(models, settings, datasets):\n",
    "    metric = \"cal_correct_over_incorrect\"\n",
    "    for model in models:\n",
    "        n_layers = MODEL_PARAMS[model][\"num_layers\"] + 1\n",
    "        for setting in settings:\n",
    "            tp_avg = np.zeros(n_layers)\n",
    "            fp_avg = np.zeros(n_layers)\n",
    "            for dataset in datasets:\n",
    "                avg = np.mean(logit_lens[model][setting][dataset][metric], axis=0)[0]\n",
    "                tp_avg += avg[0, :, -1]\n",
    "                fp_avg += avg[1, :, -1]\n",
    "            tp_avg /= len(datasets)\n",
    "            fp_avg /= len(datasets)\n",
    "\n",
    "            percent_of_final_gap = []\n",
    "            for i in range(n_layers):\n",
    "                percent_of_final_gap.append(\n",
    "                    (tp_avg[i] - fp_avg[i]) / (tp_avg[-1] - fp_avg[-1])\n",
    "                )\n",
    "\n",
    "            pofg = pd.DataFrame({\"layer\": list(range(n_layers)), \"p\": percent_of_final_gap})\n",
    "            pofg.to_csv(\n",
    "                f\"../../results/figures/attention/{model}/{setting}/percent_of_final_gap.csv\",\n",
    "                index=False,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "844ff4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_perc_of_gap_avg_over_datasets(models, [\"permuted_incorrect_labels\"], datasets_main)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b852038",
   "metadata": {},
   "source": [
    "#### 7.2) Sum of PM Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c1fa15e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_sum_of_pm_score_avg_over_pf(models, settings, datasets):\n",
    "    for model in models:\n",
    "        n_layers = MODEL_PARAMS[model][\"num_layers\"]\n",
    "        for setting in settings:\n",
    "            layer_scores = [0] * n_layers\n",
    "            for dataset in datasets:\n",
    "                df = pd.DataFrame(attention[model][setting][dataset])\n",
    "                df = df[df[\"demo_indx\"] == df[\"demo_indx\"].max()]\n",
    "                for i in range(n_layers):\n",
    "                    df_layer = df[df[\"layer_indx\"] == i]\n",
    "                    for j in range(df[\"head_indx\"].max() + 1):\n",
    "                        df_head = df_layer[df_layer[\"head_indx\"] == j]\n",
    "                        layer_scores[i] += df_head.iloc[0][\"cfs_lab_prime\"]\n",
    "                        layer_scores[i] += df_head.iloc[1][\"cfs_lab_prime\"]\n",
    "                pm_score = pd.DataFrame({\"layer\": list(range(n_layers)), \"p\": layer_scores})\n",
    "                pm_score.to_csv(\n",
    "                    f\"../../results/figures/attention/{model}/{setting}/{dataset}_sum_of_pm_scores.csv\",\n",
    "                    index=False,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bebdfca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_sum_of_pm_score_avg_over_pf(models, [\"permuted_incorrect_labels\"], [\"unnatural\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0f2f3c",
   "metadata": {},
   "source": [
    "## 8) Ablations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fae485",
   "metadata": {},
   "source": [
    "### 8.1 {Attention, MLP, Late Layer} Ablation\n",
    "\n",
    "See section 2 in notebooks/analysis/ablations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84d9ce7",
   "metadata": {},
   "source": [
    "### 8.2 Head Ablation\n",
    "\n",
    "See section 3 in notebooks/analysis/ablations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
